{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading https://files.pythonhosted.org/packages/39/11/896451b887472cdd500a96eb5aee2206564fb05faca15edea7515877b2e5/spacy-2.2.1-cp37-cp37m-win_amd64.whl (9.3MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/26/fb9d708e2570bb48f35ce8b6f796ece9b0805191eb11545697a4e9fe06bc/cymem-2.0.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting thinc<7.2.0,>=7.1.1 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/f4/ca5a5e47425ffe196916e0c8c4d59e4391157332b858a2d8b0274347299d/thinc-7.1.1-cp37-cp37m-win_amd64.whl (1.9MB)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=0.1.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/d9/dc2979367b3f850ca31c181d58e05c26e7d3c0d4e71ab8455e26db979b2c/srsly-0.1.0-cp37-cp37m-win_amd64.whl (171kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/7b/d77bc9bb101e113884b2d70a118e7ec8dcc9846a35a0e10d47ca37acdcbf/murmurhash-1.0.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/d5/7e/1981d5389b75543f950026de40a9d346e2aec7e860b2800e54e65bd46c06/blis-0.4.1-cp37-cp37m-win_amd64.whl (5.0MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.16.5)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/5a/0d1b575ed40989d74fab25723083837c220246b25f3582917135cb32453f/preshed-3.0.2-cp37-cp37m-win_amd64.whl (105kB)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in d:\\anaconda\\lib\\site-packages (from thinc<7.2.0,>=7.1.1->spacy) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Installing collected packages: cymem, srsly, wasabi, murmurhash, preshed, blis, plac, thinc, spacy\n",
      "Successfully installed blis-0.4.1 cymem-2.0.2 murmurhash-1.0.2 plac-0.9.6 preshed-3.0.2 spacy-2.2.1 srsly-0.1.0 thinc-7.1.1 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n",
      "Requirement already satisfied: spacy>=2.2.0 in d:\\anaconda\\lib\\site-packages (from en_core_web_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.16.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in d:\\anaconda\\lib\\site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.36.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.24.2)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-cp37-none-any.whl size=12019129 sha256=a3b034633201162dd9ca23bf8613b55154c46f8acd4c7ebb38e4acacbf877daa\n",
      "  Stored in directory: C:\\Users\\Sayan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-irvm70l5\\wheels\\48\\5c\\1c\\15f9d02afc8221a668d2172446dd8467b20cdb9aef80a172a4\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "from spacy.symbols import  LEMMA # POS, TAG, ORTH, add'l if desired\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, recurrent, GRU, Bidirectional #Dropout, Flatten, RNN, SimpleRNN,\n",
    "from keras.layers import Embedding, GlobalMaxPooling1D, LSTM #Conv2D, MaxPooling2D, Conv1D,\n",
    "from keras import backend as K\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import text\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    lemma = []\n",
    "    text = data['tweet']\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm') #might have to download\n",
    "    doc = nlp(\"This is a sentence.\")\n",
    "    print(doc)\n",
    "    #Marks -PRON- for Pronoun\n",
    "    for tweet in text:\n",
    "        tweet = nlp(tweet)\n",
    "        lemma.append([])\n",
    "        for token in tweet:\n",
    "            # if token != \"@USER\":\n",
    "#             print(token)\n",
    "            lemma[-1].append(token.lemma_)\n",
    "#         print(lemma)\n",
    "#         print(\"------------------------\")\n",
    "\n",
    "    indexed = []\n",
    "\n",
    "    max_len = len(max(lemma, key=len))\n",
    "    print(max_len)\n",
    "    #this is if you haven't saved the data and need to re-run it\n",
    "#     pickle.dump(lemma, open('lemma_tokens.pkl', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    #to create vocabulary\n",
    "#     Here each word is assigned a value in ascending order\n",
    "    vocabulary = {}\n",
    "    value = 1\n",
    "    for tweet in lemma:\n",
    "        for token in tweet:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = value\n",
    "                value += 1\n",
    "\n",
    "#     print(vocabulary)\n",
    "\n",
    "#     #this is if you haven't saved the data and need to re-run it\n",
    "#     pickle.dump(vocabulary, open('vocabulary.pkl', 'wb'))\n",
    "\n",
    "#     # previously created dictionary of vocabulary\n",
    "#     vocabulary = pickle.load(open('vocabulary.pkl', 'rb'))\n",
    "\n",
    "    #turn tokenized tweets into integers\n",
    "    for tweet in lemma:\n",
    "        row = []\n",
    "        for token in tweet:\n",
    "            if token in vocabulary: #this ignores new vocabulary from test or dev\n",
    "                row.append(vocabulary[token]) #is extend better here?\n",
    "            #possibly have to add an else here to add the word to vocabulary dictionary\n",
    "            #and then run the rest of it for new words from dev or test\n",
    "        while len(row) < max_len:\n",
    "            row.append(0)\n",
    "        indexed.append(row)\n",
    "\n",
    "    inputs = np.array(indexed)\n",
    "    print(inputs.shape)\n",
    "    print(inputs[:10])\n",
    "#         #this is if you haven't saved the data and need to re-run it\n",
    "#     pickle.dump(indexed, open('lemma_indexed.pkl', 'wb'))\n",
    "\n",
    "#     # won't work right, change to whatever column header is\n",
    "    outputs = data.loc[: , 'subtask_a']\n",
    "    print(outputs[:20])\n",
    "    outputs = outputs.values.copy()\n",
    "    print(outputs[:20])\n",
    "\n",
    "    return inputs, outputs, vocabulary, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_data: pd.DataFrame,\n",
    "                      dev_data: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Training Starts\")\n",
    "    #this is if you haven't saved the data and need to re-run it\n",
    "    train_inputs, train_outputs, vocabulary, max_len = preprocessing(train_data)\n",
    "\n",
    "\n",
    "#     train_inputs = np.array(pickle.load(open('lemma_indexed.pkl', 'rb')))\n",
    "    print(\"train_inputs\")\n",
    "    print(train_inputs[:10])\n",
    "    print(train_inputs.shape)\n",
    "\n",
    "#     train_outputs = train_inputs.loc[:, 'subtask_a'].values.copy() #train_data\n",
    "    print(\"train_outputs\")\n",
    "    print(train_outputs[:10])\n",
    "    print(train_outputs.shape)\n",
    "#     vocabulary = pickle.load(open('vocabulary.pkl', 'rb')) #vocab above but this is better version?\n",
    "#     print(vocabulary)\n",
    "\n",
    "    #play with more layers\n",
    "    rnn = Sequential()\n",
    "    rnn.add(Embedding(input_dim = len(vocabulary) +1, output_dim = 75, input_length=max_len))\n",
    "    rnn.add(Flatten())\n",
    "    rnn.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    print(rnn.summary())\n",
    "#     rnn.add(Bidirectional(GRU(128, activation='tanh', return_sequences=True)))\n",
    "#     rnn.add(GlobalMaxPooling1D())\n",
    "#     rnn.add(Dense(1, activation='sigmoid'))\n",
    "#     rnn.compile(loss='binary_crossentropy',\n",
    "#              optimizer= 'adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    rnn.fit(x = train_inputs, y = train_outputs, epochs=10, verbose=0)\n",
    "    loss, accuracy = rnn.evaluate(train_inputs, train_outputs, verbose=0)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "#     dev_inputs, dev_outputs, vocabulary, max_len = preprocessing(dev_data)\n",
    "#     predictions = np.round(rnn.predict(dev_inputs)).astype(int)\n",
    "\n",
    "\n",
    "#     dev_predictions = dev_data.copy()\n",
    "\n",
    "#     dev_predictions[offense] = predictions\n",
    "#     print(\"devpredict\")\n",
    "#     print(dev_predictions)\n",
    "\n",
    "#     return dev_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts\n",
      "This is a sentence.\n",
      "112\n",
      "(13240, 112)\n",
      "[[  1   2   3 ...   0   0   0]\n",
      " [  1   1  15 ...   0   0   0]\n",
      " [ 27  13  28 ...   0   0   0]\n",
      " ...\n",
      " [  1  13  77 ...   0   0   0]\n",
      " [  1  99 100 ...   0   0   0]\n",
      " [  1 102 103 ...   0   0   0]]\n",
      "0     1\n",
      "1     1\n",
      "2     0\n",
      "3     1\n",
      "4     0\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     0\n",
      "9     1\n",
      "10    0\n",
      "11    0\n",
      "12    1\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    1\n",
      "Name: subtask_a, dtype: int64\n",
      "[1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1]\n",
      "train_inputs\n",
      "[[  1   2   3 ...   0   0   0]\n",
      " [  1   1  15 ...   0   0   0]\n",
      " [ 27  13  28 ...   0   0   0]\n",
      " ...\n",
      " [  1  13  77 ...   0   0   0]\n",
      " [  1  99 100 ...   0   0   0]\n",
      " [  1 102 103 ...   0   0   0]]\n",
      "(13240, 112)\n",
      "train_outputs\n",
      "[1 1 0 1 0 1 1 1 0 1]\n",
      "(13240,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 112, 75)           1539675   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 8401      \n",
      "=================================================================\n",
      "Total params: 1,548,076\n",
      "Trainable params: 1,548,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training Accuracy: 99.856495\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # reads train and dev data into Pandas data frames\n",
    "#     read_csv_kwargs = dict(sep=\"\\t\",\n",
    "#                            converters={e: offense_to_int.get for e in offense})\n",
    "#     train_and_dev = pd.read_csv(\"offenseval-training-v1.tsv\", **read_csv_kwargs)\n",
    "#     len_of_data = len(train_and_dev)\n",
    "\n",
    "\n",
    "#     #possibly better way to split\n",
    "# #     from sklearn.model_selection import train_test_split\n",
    "# # train, test = train_test_split(df, test_size=0.33, random_state=42)\n",
    "#     dev_len = int(len_of_data/4)\n",
    "#     train_data = train_and_dev[:dev_len]\n",
    "#     print(train_data.head)\n",
    "#     dev_data = train_and_dev[dev_len:]\n",
    "#     print(dev_data.head)\n",
    "    train_data = pd.read_csv('./training-v1/offenseval-training-v1.tsv', sep='\\t', header=0)\n",
    "    col_Names=[\"tweet\", \"subtask_a\", \"subtask_b\", \"subtask_c\"]\n",
    "    dev_data = pd.read_csv('./trial-data/offenseval-trial.txt', sep='\\t', names=col_Names)\n",
    "    cleanup_nums = {\"subtask_a\":     {\"OFF\": 1, \"NOT\": 0}}\n",
    "    train_data.replace(cleanup_nums, inplace=True)\n",
    "    dev_data.replace(cleanup_nums, inplace=True)\n",
    "#     preprocessing(train_data)\n",
    "#     makes predictions on the dev set\n",
    "    dev_predictions = train_and_predict(train_data, dev_data)\n",
    "#     print(\"dev_predictions\")\n",
    "#     print(dev_predictions[:10])\n",
    "#     print(\"type: \" + str(type(dev_predictions)))\n",
    "\n",
    "#     dev_predictions_formatted = dev_predictions\n",
    "\n",
    "\n",
    "# #     The prediction format is a comma-delimited text file with a *.csv extension which\n",
    "# #     should contain two columns: (1) the sample ID, and (2) the sample label.\n",
    "# #      You can name the file anything you like, so long as it has a .csv extension.\n",
    "# #      The order of the entries is not important, but there must be one prediction for each ID.\n",
    "# #       The CSV file should not have a header row.\n",
    "\n",
    "# # saves predictions and prints out multi-label accuracy\n",
    "# #     index = ids?\n",
    "\n",
    "#     dev_predictions = pd.DataFrame(dev_predictions)\n",
    "#     dev_predictions = dev_predictions.loc[:, ['id', 'tweet', 'subtask_a']]\n",
    "#     dev_predictions['gold_label'] = dev_data[offense]\n",
    "\n",
    "#     dev_predictions.to_csv(\"error_analysis.csv\", index=False, header= True) #, sep=\"\\t\"\n",
    "#     print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
    "#         dev_data[offense], dev_predictions[offense])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": #makes it easier to pull these in from elsewhere b/c name == main only here\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
