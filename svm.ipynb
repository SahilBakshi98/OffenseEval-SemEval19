{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "--LyPKw5sbCP",
    "outputId": "317d7b5e-1a7d-4b21-cccc-5af9a567abf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sahil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sahil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Preprocessor import DataReader, Preprocessor, Vectorizer, Classifier\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import nltk\n",
    "nltk.download(['stopwords','wordnet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wm9C_I7akuZ-"
   },
   "source": [
    "Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0LJucFgpKJzH",
    "outputId": "a440f09a-e584-4aae-8fe6-5d543d6e7ae1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Data: 13241it [00:00, 294997.29it/s]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 395it [00:00, 3948.36it/s]\u001b[A\n",
      "Tokenization: 767it [00:00, 3874.53it/s]\u001b[A\n",
      "Tokenization: 1146it [00:00, 3847.64it/s]\u001b[A\n",
      "Tokenization: 1480it [00:00, 3678.58it/s]\u001b[A\n",
      "Tokenization: 1851it [00:00, 3687.28it/s]\u001b[A\n",
      "Tokenization: 2238it [00:00, 3738.82it/s]\u001b[A\n",
      "Tokenization: 2567it [00:00, 3557.88it/s]\u001b[A\n",
      "Tokenization: 2893it [00:00, 3176.03it/s]\u001b[A\n",
      "Tokenization: 3198it [00:00, 3018.67it/s]\u001b[A\n",
      "Tokenization: 3493it [00:01, 2983.32it/s]\u001b[A\n",
      "Tokenization: 3868it [00:01, 3177.86it/s]\u001b[A\n",
      "Tokenization: 4318it [00:01, 3482.28it/s]\u001b[A\n",
      "Tokenization: 4725it [00:01, 3638.23it/s]\u001b[A\n",
      "Tokenization: 5140it [00:01, 3774.05it/s]\u001b[A\n",
      "Tokenization: 5525it [00:01, 3747.98it/s]\u001b[A\n",
      "Tokenization: 5937it [00:01, 3852.13it/s]\u001b[A\n",
      "Tokenization: 6327it [00:01, 3748.07it/s]\u001b[A\n",
      "Tokenization: 6706it [00:01, 3546.89it/s]\u001b[A\n",
      "Tokenization: 7066it [00:01, 3446.65it/s]\u001b[A\n",
      "Tokenization: 7415it [00:02, 3181.37it/s]\u001b[A\n",
      "Tokenization: 7740it [00:02, 3182.94it/s]\u001b[A\n",
      "Tokenization: 8171it [00:02, 3448.12it/s]\u001b[A\n",
      "Tokenization: 8584it [00:02, 3627.56it/s]\u001b[A\n",
      "Tokenization: 8988it [00:02, 3740.50it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:02<00:05,  2.60s/it]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 3206it [00:00, 32056.44it/s]\u001b[A\n",
      "Stopwords Removal: 7541it [00:00, 34772.29it/s]\u001b[A\n",
      "Preprocessing:  67%|██████▋   | 2/3 [00:02<00:01,  1.89s/it]\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 60it [00:00, 581.88it/s]\u001b[A\n",
      "Lemmatization: 101it [00:00, 515.79it/s]\u001b[A\n",
      "Lemmatization: 166it [00:00, 548.14it/s]\u001b[A\n",
      "Lemmatization: 226it [00:00, 562.50it/s]\u001b[A\n",
      "Lemmatization: 286it [00:00, 568.81it/s]\u001b[A\n",
      "Lemmatization: 339it [00:00, 555.18it/s]\u001b[A\n",
      "Lemmatization: 405it [00:00, 582.70it/s]\u001b[A\n",
      "Lemmatization: 460it [00:00, 550.62it/s]\u001b[A\n",
      "Lemmatization: 523it [00:00, 565.13it/s]\u001b[A\n",
      "Lemmatization: 587it [00:01, 583.30it/s]\u001b[A\n",
      "Lemmatization: 656it [00:01, 609.82it/s]\u001b[A\n",
      "Lemmatization: 717it [00:01, 599.70it/s]\u001b[A\n",
      "Lemmatization: 787it [00:01, 623.49it/s]\u001b[A\n",
      "Lemmatization: 850it [00:01, 622.12it/s]\u001b[A\n",
      "Lemmatization: 914it [00:01, 623.00it/s]\u001b[A\n",
      "Lemmatization: 1000it [00:01, 678.50it/s]\u001b[A\n",
      "Lemmatization: 1083it [00:01, 714.84it/s]\u001b[A\n",
      "Lemmatization: 1161it [00:01, 730.14it/s]\u001b[A\n",
      "Lemmatization: 1242it [00:01, 751.12it/s]\u001b[A\n",
      "Lemmatization: 1325it [00:02, 766.20it/s]\u001b[A\n",
      "Lemmatization: 1408it [00:02, 780.92it/s]\u001b[A\n",
      "Lemmatization: 1487it [00:02, 775.10it/s]\u001b[A\n",
      "Lemmatization: 1565it [00:02, 757.83it/s]\u001b[A\n",
      "Lemmatization: 1642it [00:02, 746.55it/s]\u001b[A\n",
      "Lemmatization: 1727it [00:02, 773.93it/s]\u001b[A\n",
      "Lemmatization: 1811it [00:02, 789.39it/s]\u001b[A\n",
      "Lemmatization: 1893it [00:02, 792.67it/s]\u001b[A\n",
      "Lemmatization: 1973it [00:02, 783.74it/s]\u001b[A\n",
      "Lemmatization: 2052it [00:02, 740.97it/s]\u001b[A\n",
      "Lemmatization: 2127it [00:03, 726.91it/s]\u001b[A\n",
      "Lemmatization: 2201it [00:03, 730.23it/s]\u001b[A\n",
      "Lemmatization: 2275it [00:03, 713.86it/s]\u001b[A\n",
      "Lemmatization: 2361it [00:03, 751.47it/s]\u001b[A\n",
      "Lemmatization: 2452it [00:03, 791.98it/s]\u001b[A\n",
      "Lemmatization: 2536it [00:03, 799.63it/s]\u001b[A\n",
      "Lemmatization: 2624it [00:03, 821.31it/s]\u001b[A\n",
      "Lemmatization: 2718it [00:03, 849.70it/s]\u001b[A\n",
      "Lemmatization: 2804it [00:03, 827.90it/s]\u001b[A\n",
      "Lemmatization: 2888it [00:04, 769.04it/s]\u001b[A\n",
      "Lemmatization: 2967it [00:04, 705.66it/s]\u001b[A\n",
      "Lemmatization: 3040it [00:04, 708.73it/s]\u001b[A\n",
      "Lemmatization: 3122it [00:04, 736.51it/s]\u001b[A\n",
      "Lemmatization: 3197it [00:04, 728.28it/s]\u001b[A\n",
      "Lemmatization: 3271it [00:04, 707.20it/s]\u001b[A\n",
      "Lemmatization: 3343it [00:04, 677.38it/s]\u001b[A\n",
      "Lemmatization: 3412it [00:04, 623.16it/s]\u001b[A\n",
      "Lemmatization: 3476it [00:04, 587.18it/s]\u001b[A\n",
      "Lemmatization: 3537it [00:05, 593.12it/s]\u001b[A\n",
      "Lemmatization: 3612it [00:05, 631.54it/s]\u001b[A\n",
      "Lemmatization: 3677it [00:05, 597.08it/s]\u001b[A\n",
      "Lemmatization: 3755it [00:05, 641.81it/s]\u001b[A\n",
      "Lemmatization: 3829it [00:05, 666.07it/s]\u001b[A\n",
      "Lemmatization: 3898it [00:05, 659.20it/s]\u001b[A\n",
      "Lemmatization: 3966it [00:05, 605.42it/s]\u001b[A\n",
      "Lemmatization: 4029it [00:05, 607.87it/s]\u001b[A\n",
      "Lemmatization: 4091it [00:05, 598.43it/s]\u001b[A\n",
      "Lemmatization: 4153it [00:06, 603.82it/s]\u001b[A\n",
      "Lemmatization: 4214it [00:06, 566.92it/s]\u001b[A\n",
      "Lemmatization: 4283it [00:06, 596.99it/s]\u001b[A\n",
      "Lemmatization: 4350it [00:06, 612.94it/s]\u001b[A\n",
      "Lemmatization: 4421it [00:06, 637.07it/s]\u001b[A\n",
      "Lemmatization: 4486it [00:06, 595.29it/s]\u001b[A\n",
      "Lemmatization: 4547it [00:06, 573.38it/s]\u001b[A\n",
      "Lemmatization: 4606it [00:06, 547.50it/s]\u001b[A\n",
      "Lemmatization: 4662it [00:06, 545.69it/s]\u001b[A\n",
      "Lemmatization: 4722it [00:07, 557.26it/s]\u001b[A\n",
      "Lemmatization: 4779it [00:07, 510.03it/s]\u001b[A\n",
      "Lemmatization: 4852it [00:07, 560.38it/s]\u001b[A\n",
      "Lemmatization: 4913it [00:07, 572.08it/s]\u001b[A\n",
      "Lemmatization: 4972it [00:07, 542.34it/s]\u001b[A\n",
      "Lemmatization: 5028it [00:07, 503.13it/s]\u001b[A\n",
      "Lemmatization: 5111it [00:07, 569.87it/s]\u001b[A\n",
      "Lemmatization: 5173it [00:07, 542.39it/s]\u001b[A\n",
      "Lemmatization: 5231it [00:07, 515.59it/s]\u001b[A\n",
      "Lemmatization: 5297it [00:08, 550.25it/s]\u001b[A\n",
      "Lemmatization: 5376it [00:08, 604.66it/s]\u001b[A\n",
      "Lemmatization: 5447it [00:08, 630.96it/s]\u001b[A\n",
      "Lemmatization: 5531it [00:08, 676.77it/s]\u001b[A\n",
      "Lemmatization: 5607it [00:08, 699.63it/s]\u001b[A\n",
      "Lemmatization: 5687it [00:08, 726.84it/s]\u001b[A\n",
      "Lemmatization: 5762it [00:08, 692.94it/s]\u001b[A\n",
      "Lemmatization: 5833it [00:08, 665.68it/s]\u001b[A\n",
      "Lemmatization: 5901it [00:08, 663.40it/s]\u001b[A\n",
      "Lemmatization: 5969it [00:09, 648.42it/s]\u001b[A\n",
      "Lemmatization: 6040it [00:09, 665.00it/s]\u001b[A\n",
      "Lemmatization: 6108it [00:09, 609.49it/s]\u001b[A\n",
      "Lemmatization: 6171it [00:09, 602.87it/s]\u001b[A\n",
      "Lemmatization: 6243it [00:09, 633.39it/s]\u001b[A\n",
      "Lemmatization: 6309it [00:09, 639.22it/s]\u001b[A\n",
      "Lemmatization: 6376it [00:09, 644.60it/s]\u001b[A\n",
      "Lemmatization: 6442it [00:09, 629.93it/s]\u001b[A\n",
      "Lemmatization: 6506it [00:09, 621.13it/s]\u001b[A\n",
      "Lemmatization: 6577it [00:10, 642.08it/s]\u001b[A\n",
      "Lemmatization: 6643it [00:10, 644.41it/s]\u001b[A\n",
      "Lemmatization: 6714it [00:10, 661.89it/s]\u001b[A\n",
      "Lemmatization: 6786it [00:10, 671.96it/s]\u001b[A\n",
      "Lemmatization: 6854it [00:10, 616.30it/s]\u001b[A\n",
      "Lemmatization: 6917it [00:10, 614.22it/s]\u001b[A\n",
      "Lemmatization: 6980it [00:10, 612.59it/s]\u001b[A\n",
      "Lemmatization: 7044it [00:10, 619.65it/s]\u001b[A\n",
      "Lemmatization: 7116it [00:10, 644.30it/s]\u001b[A\n",
      "Lemmatization: 7181it [00:10, 639.37it/s]\u001b[A\n",
      "Lemmatization: 7246it [00:11, 641.51it/s]\u001b[A\n",
      "Lemmatization: 7311it [00:11, 643.87it/s]\u001b[A\n",
      "Lemmatization: 7379it [00:11, 647.47it/s]\u001b[A\n",
      "Lemmatization: 7445it [00:11, 648.31it/s]\u001b[A\n",
      "Lemmatization: 7510it [00:11, 640.37it/s]\u001b[A\n",
      "Lemmatization: 7575it [00:11, 642.23it/s]\u001b[A\n",
      "Lemmatization: 7643it [00:11, 649.02it/s]\u001b[A\n",
      "Lemmatization: 7708it [00:11, 618.72it/s]\u001b[A\n",
      "Lemmatization: 7771it [00:11, 614.75it/s]\u001b[A\n",
      "Lemmatization: 7846it [00:11, 648.85it/s]\u001b[A\n",
      "Lemmatization: 7912it [00:12, 645.70it/s]\u001b[A\n",
      "Lemmatization: 7978it [00:12, 620.81it/s]\u001b[A\n",
      "Lemmatization: 8041it [00:12, 616.05it/s]\u001b[A\n",
      "Lemmatization: 8123it [00:12, 663.11it/s]\u001b[A\n",
      "Lemmatization: 8191it [00:12, 663.44it/s]\u001b[A\n",
      "Lemmatization: 8259it [00:12, 637.41it/s]\u001b[A\n",
      "Lemmatization: 8324it [00:12, 618.00it/s]\u001b[A\n",
      "Lemmatization: 8387it [00:12, 593.88it/s]\u001b[A\n",
      "Lemmatization: 8448it [00:12, 556.57it/s]\u001b[A\n",
      "Lemmatization: 8505it [00:13, 556.79it/s]\u001b[A\n",
      "Lemmatization: 8575it [00:13, 590.90it/s]\u001b[A\n",
      "Lemmatization: 8653it [00:13, 636.12it/s]\u001b[A\n",
      "Lemmatization: 8726it [00:13, 660.64it/s]\u001b[A\n",
      "Lemmatization: 8794it [00:13, 659.78it/s]\u001b[A\n",
      "Lemmatization: 8864it [00:13, 669.41it/s]\u001b[A\n",
      "Lemmatization: 8934it [00:13, 678.30it/s]\u001b[A\n",
      "Lemmatization: 9015it [00:13, 712.53it/s]\u001b[A\n",
      "Lemmatization: 9088it [00:13, 685.36it/s]\u001b[A\n",
      "Lemmatization: 9158it [00:14, 673.29it/s]\u001b[A\n",
      "Lemmatization: 9226it [00:14, 670.70it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:17<00:00,  5.58s/it]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 291it [00:00, 2903.97it/s]\u001b[A\n",
      "Tokenization: 630it [00:00, 3034.03it/s]\u001b[A\n",
      "Tokenization: 1038it [00:00, 3285.55it/s]\u001b[A\n",
      "Tokenization: 1458it [00:00, 3514.84it/s]\u001b[A\n",
      "Tokenization: 1854it [00:00, 3631.94it/s]\u001b[A\n",
      "Tokenization: 2254it [00:00, 3734.28it/s]\u001b[A\n",
      "Tokenization: 2631it [00:00, 3740.91it/s]\u001b[A\n",
      "Tokenization: 3002it [00:00, 3730.18it/s]\u001b[A\n",
      "Tokenization: 3396it [00:00, 3790.73it/s]\u001b[A\n",
      "Tokenization: 3767it [00:01, 3765.73it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:01<00:02,  1.06s/it]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 3950it [00:00, 39496.27it/s]\u001b[A\n",
      "Preprocessing:  67%|██████▋   | 2/3 [00:01<00:00,  1.29it/s]\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 76it [00:00, 751.70it/s]\u001b[A\n",
      "Lemmatization: 144it [00:00, 725.85it/s]\u001b[A\n",
      "Lemmatization: 215it [00:00, 716.99it/s]\u001b[A\n",
      "Lemmatization: 289it [00:00, 720.32it/s]\u001b[A\n",
      "Lemmatization: 362it [00:00, 718.02it/s]\u001b[A\n",
      "Lemmatization: 427it [00:00, 692.95it/s]\u001b[A\n",
      "Lemmatization: 512it [00:00, 729.70it/s]\u001b[A\n",
      "Lemmatization: 580it [00:00, 692.98it/s]\u001b[A\n",
      "Lemmatization: 646it [00:00, 640.90it/s]\u001b[A\n",
      "Lemmatization: 724it [00:01, 674.49it/s]\u001b[A\n",
      "Lemmatization: 800it [00:01, 693.18it/s]\u001b[A\n",
      "Lemmatization: 869it [00:01, 669.63it/s]\u001b[A\n",
      "Lemmatization: 942it [00:01, 686.33it/s]\u001b[A\n",
      "Lemmatization: 1011it [00:01, 673.78it/s]\u001b[A\n",
      "Lemmatization: 1094it [00:01, 711.17it/s]\u001b[A\n",
      "Lemmatization: 1171it [00:01, 726.73it/s]\u001b[A\n",
      "Lemmatization: 1246it [00:01, 732.22it/s]\u001b[A\n",
      "Lemmatization: 1322it [00:01, 736.57it/s]\u001b[A\n",
      "Lemmatization: 1399it [00:01, 744.57it/s]\u001b[A\n",
      "Lemmatization: 1477it [00:02, 750.31it/s]\u001b[A\n",
      "Lemmatization: 1553it [00:02, 735.80it/s]\u001b[A\n",
      "Lemmatization: 1627it [00:02, 728.62it/s]\u001b[A\n",
      "Lemmatization: 1704it [00:02, 739.97it/s]\u001b[A\n",
      "Lemmatization: 1779it [00:02, 741.11it/s]\u001b[A\n",
      "Lemmatization: 1854it [00:02, 725.63it/s]\u001b[A\n",
      "Lemmatization: 1927it [00:02, 701.16it/s]\u001b[A\n",
      "Lemmatization: 2008it [00:02, 723.95it/s]\u001b[A\n",
      "Lemmatization: 2081it [00:02, 717.82it/s]\u001b[A\n",
      "Lemmatization: 2154it [00:03, 682.95it/s]\u001b[A\n",
      "Lemmatization: 2223it [00:03, 654.83it/s]\u001b[A\n",
      "Lemmatization: 2292it [00:03, 663.45it/s]\u001b[A\n",
      "Lemmatization: 2367it [00:03, 685.47it/s]\u001b[A\n",
      "Lemmatization: 2437it [00:03, 679.40it/s]\u001b[A\n",
      "Lemmatization: 2509it [00:03, 690.36it/s]\u001b[A\n",
      "Lemmatization: 2579it [00:03, 684.41it/s]\u001b[A\n",
      "Lemmatization: 2648it [00:03, 650.95it/s]\u001b[A\n",
      "Lemmatization: 2714it [00:03, 624.87it/s]\u001b[A\n",
      "Lemmatization: 2785it [00:03, 647.58it/s]\u001b[A\n",
      "Lemmatization: 2868it [00:04, 692.59it/s]\u001b[A\n",
      "Lemmatization: 2939it [00:04, 678.75it/s]\u001b[A\n",
      "Lemmatization: 3008it [00:04, 655.25it/s]\u001b[A\n",
      "Lemmatization: 3079it [00:04, 669.01it/s]\u001b[A\n",
      "Lemmatization: 3157it [00:04, 696.63it/s]\u001b[A\n",
      "Lemmatization: 3228it [00:04, 662.51it/s]\u001b[A\n",
      "Lemmatization: 3296it [00:04, 653.27it/s]\u001b[A\n",
      "Lemmatization: 3369it [00:04, 672.64it/s]\u001b[A\n",
      "Lemmatization: 3448it [00:04, 699.69it/s]\u001b[A\n",
      "Lemmatization: 3519it [00:05, 664.56it/s]\u001b[A\n",
      "Lemmatization: 3593it [00:05, 678.96it/s]\u001b[A\n",
      "Lemmatization: 3662it [00:05, 679.51it/s]\u001b[A\n",
      "Lemmatization: 3731it [00:05, 674.71it/s]\u001b[A\n",
      "Lemmatization: 3799it [00:05, 630.98it/s]\u001b[A\n",
      "Lemmatization: 3863it [00:05, 626.88it/s]\u001b[A\n",
      "Lemmatization: 3935it [00:05, 650.20it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:06<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 9268/9268 [00:00<00:00, 44633.17it/s]\n",
      "Finalizing: 100%|██████████| 9268/9268 [00:02<00:00, 3376.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 3972/3972 [00:00<00:00, 43327.85it/s]\n",
      "Finalizing: 100%|██████████| 3972/3972 [00:01<00:00, 3471.76it/s]\n",
      "/home/sahil/anaconda3/envs/ire_project/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Training Data Score: 0.8404186447993095\n",
      "Test Data Score: 0.7288519637462235\n",
      "Precision:  0.6423766816143498\n",
      "Recall:  0.43050338091660406\n",
      "F1 Score 0.5155195681511471\n"
     ]
    }
   ],
   "source": [
    "final_test_a = []\n",
    "final_train_a = []\n",
    "dr_a = DataReader('./datasets/training-v1/offenseval-training-v1.tsv','A')\n",
    "data_a,labels_a = dr_a.get_labelled_data()\n",
    "Xa_train,Xa_test,Ya_train,Ya_test = split(data_a,labels_a,test_size=0.3)\n",
    "preprocessors_a = [('remove_stopwords','lemmatize')]\n",
    "vectorizers_a = ['glove']\n",
    "classifiers_a = [('SVC',{'C':10,'kernel':'rbf'})]\n",
    "preprocessors_a[0] = Preprocessor(preprocessors_a[0])\n",
    "vectorizers_a[0] = Vectorizer(vectorizers_a[0])\n",
    "classifiers_a[0] = Classifier(*classifiers_a[0])\n",
    "Xa_train = preprocessors_a[0].clean(Xa_train)\n",
    "Xa_test = preprocessors_a[0].clean(Xa_test)\n",
    "final_train_a.append(vectorizers_a[0].vectorize(Xa_train))\n",
    "final_test_a.append(vectorizers_a[0].vectorize(Xa_test))\n",
    "classifiers_a[0].fit(final_train_a[0],Ya_train)\n",
    "print (\"Done\")\n",
    "classifiers_a[0].score(final_train_a[0], Ya_train, final_test_a[0], Ya_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0Xe1xbGkrLU"
   },
   "source": [
    "Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RgMwPKljkz2L",
    "outputId": "cde11448-7a22-40e0-86e9-029c1b9fd330",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Data: 13241it [00:00, 279375.52it/s]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 350it [00:00, 3496.42it/s]\u001b[A\n",
      "Tokenization: 737it [00:00, 3599.35it/s]\u001b[A\n",
      "Tokenization: 1131it [00:00, 3692.53it/s]\u001b[A\n",
      "Tokenization: 1489it [00:00, 3657.29it/s]\u001b[A\n",
      "Tokenization: 1843it [00:00, 3620.53it/s]\u001b[A\n",
      "Tokenization: 2206it [00:00, 3623.12it/s]\u001b[A\n",
      "Tokenization: 2528it [00:00, 3456.68it/s]\u001b[A\n",
      "Tokenization: 2847it [00:00, 3344.60it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:00<00:01,  1.15it/s]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 3080it [00:00, 39401.15it/s]\u001b[A\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 59it [00:00, 571.34it/s]\u001b[A\n",
      "Lemmatization: 109it [00:00, 547.18it/s]\u001b[A\n",
      "Lemmatization: 180it [00:00, 586.87it/s]\u001b[A\n",
      "Lemmatization: 244it [00:00, 601.16it/s]\u001b[A\n",
      "Lemmatization: 302it [00:00, 590.06it/s]\u001b[A\n",
      "Lemmatization: 354it [00:00, 564.63it/s]\u001b[A\n",
      "Lemmatization: 417it [00:00, 582.67it/s]\u001b[A\n",
      "Lemmatization: 484it [00:00, 605.56it/s]\u001b[A\n",
      "Lemmatization: 549it [00:00, 617.01it/s]\u001b[A\n",
      "Lemmatization: 609it [00:01, 597.46it/s]\u001b[A\n",
      "Lemmatization: 669it [00:01, 598.05it/s]\u001b[A\n",
      "Lemmatization: 738it [00:01, 622.39it/s]\u001b[A\n",
      "Lemmatization: 804it [00:01, 632.13it/s]\u001b[A\n",
      "Lemmatization: 868it [00:01, 574.52it/s]\u001b[A\n",
      "Lemmatization: 929it [00:01, 584.50it/s]\u001b[A\n",
      "Lemmatization: 998it [00:01, 608.65it/s]\u001b[A\n",
      "Lemmatization: 1065it [00:01, 625.76it/s]\u001b[A\n",
      "Lemmatization: 1129it [00:01, 619.25it/s]\u001b[A\n",
      "Lemmatization: 1201it [00:01, 644.96it/s]\u001b[A\n",
      "Lemmatization: 1269it [00:02, 650.77it/s]\u001b[A\n",
      "Lemmatization: 1335it [00:02, 632.93it/s]\u001b[A\n",
      "Lemmatization: 1399it [00:02, 602.95it/s]\u001b[A\n",
      "Lemmatization: 1460it [00:02, 594.01it/s]\u001b[A\n",
      "Lemmatization: 1524it [00:02, 606.80it/s]\u001b[A\n",
      "Lemmatization: 1586it [00:02, 605.39it/s]\u001b[A\n",
      "Lemmatization: 1648it [00:02, 608.63it/s]\u001b[A\n",
      "Lemmatization: 1710it [00:02, 571.59it/s]\u001b[A\n",
      "Lemmatization: 1768it [00:02, 532.17it/s]\u001b[A\n",
      "Lemmatization: 1839it [00:03, 572.99it/s]\u001b[A\n",
      "Lemmatization: 1909it [00:03, 605.55it/s]\u001b[A\n",
      "Lemmatization: 1972it [00:03, 603.47it/s]\u001b[A\n",
      "Lemmatization: 2034it [00:03, 601.31it/s]\u001b[A\n",
      "Lemmatization: 2095it [00:03, 593.89it/s]\u001b[A\n",
      "Lemmatization: 2155it [00:03, 580.75it/s]\u001b[A\n",
      "Lemmatization: 2214it [00:03, 579.71it/s]\u001b[A\n",
      "Lemmatization: 2273it [00:03, 567.75it/s]\u001b[A\n",
      "Lemmatization: 2331it [00:03, 562.75it/s]\u001b[A\n",
      "Lemmatization: 2406it [00:03, 607.36it/s]\u001b[A\n",
      "Lemmatization: 2468it [00:04, 596.25it/s]\u001b[A\n",
      "Lemmatization: 2529it [00:04, 504.78it/s]\u001b[A\n",
      "Lemmatization: 2583it [00:04, 511.35it/s]\u001b[A\n",
      "Lemmatization: 2648it [00:04, 545.63it/s]\u001b[A\n",
      "Lemmatization: 2714it [00:04, 574.67it/s]\u001b[A\n",
      "Lemmatization: 2774it [00:04, 571.61it/s]\u001b[A\n",
      "Lemmatization: 2837it [00:04, 585.56it/s]\u001b[A\n",
      "Lemmatization: 2897it [00:04, 557.60it/s]\u001b[A\n",
      "Lemmatization: 2954it [00:05, 532.79it/s]\u001b[A\n",
      "Lemmatization: 3019it [00:05, 560.36it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:06<00:00,  1.40s/it]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 415it [00:00, 4147.43it/s]\u001b[A\n",
      "Tokenization: 796it [00:00, 4037.79it/s]\u001b[A\n",
      "Tokenization: 1137it [00:00, 3823.62it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:00<00:00,  2.69it/s]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 1320it [00:00, 32958.19it/s]\u001b[A\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 58it [00:00, 579.39it/s]\u001b[A\n",
      "Lemmatization: 106it [00:00, 545.28it/s]\u001b[A\n",
      "Lemmatization: 146it [00:00, 488.38it/s]\u001b[A\n",
      "Lemmatization: 196it [00:00, 489.66it/s]\u001b[A\n",
      "Lemmatization: 259it [00:00, 521.73it/s]\u001b[A\n",
      "Lemmatization: 326it [00:00, 557.77it/s]\u001b[A\n",
      "Lemmatization: 388it [00:00, 571.26it/s]\u001b[A\n",
      "Lemmatization: 460it [00:00, 606.34it/s]\u001b[A\n",
      "Lemmatization: 519it [00:00, 577.87it/s]\u001b[A\n",
      "Lemmatization: 576it [00:01, 560.99it/s]\u001b[A\n",
      "Lemmatization: 632it [00:01, 540.92it/s]\u001b[A\n",
      "Lemmatization: 700it [00:01, 575.60it/s]\u001b[A\n",
      "Lemmatization: 777it [00:01, 619.83it/s]\u001b[A\n",
      "Lemmatization: 856it [00:01, 662.44it/s]\u001b[A\n",
      "Lemmatization: 930it [00:01, 683.81it/s]\u001b[A\n",
      "Lemmatization: 1006it [00:01, 702.57it/s]\u001b[A\n",
      "Lemmatization: 1079it [00:01, 708.19it/s]\u001b[A\n",
      "Lemmatization: 1151it [00:01, 695.30it/s]\u001b[A\n",
      "Lemmatization: 1230it [00:01, 719.98it/s]\u001b[A\n",
      "Lemmatization: 1303it [00:02, 644.84it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 3080/3080 [00:00<00:00, 18587.71it/s]\n",
      "Finalizing: 100%|██████████| 3080/3080 [00:01<00:00, 2403.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 1320/1320 [00:00<00:00, 33853.16it/s]\n",
      "Finalizing: 100%|██████████| 1320/1320 [00:00<00:00, 3577.35it/s]\n",
      "/home/sahil/anaconda3/envs/ire_project/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Training Data Score: 0.8964285714285715\n",
      "Test Data Score: 0.8856060606060606\n",
      "Precision:  0.8862774829416225\n",
      "Recall:  0.9991452991452991\n",
      "F1 Score 0.9393330654881479\n"
     ]
    }
   ],
   "source": [
    "final_test_b = []\n",
    "final_train_b = []\n",
    "dr_b = DataReader('./datasets/training-v1/offenseval-training-v1.tsv','B')\n",
    "data_b,labels_b = dr_b.get_labelled_data()\n",
    "Xb_train,Xb_test,Yb_train,Yb_test = split(data_b,labels_b,test_size=0.3)\n",
    "preprocessors_b = [('remove_stopwords','lemmatize')]\n",
    "vectorizers_b = ['glove']\n",
    "classifiers_b = [('SVC',{'C':10,'kernel':'rbf'})]\n",
    "preprocessors_b[0] = Preprocessor(preprocessors_b[0])\n",
    "vectorizers_b[0] = Vectorizer(vectorizers_b[0])\n",
    "classifiers_b[0] = Classifier(*classifiers_b[0])\n",
    "Xb_train = preprocessors_b[0].clean(Xb_train)\n",
    "Xb_test = preprocessors_b[0].clean(Xb_test)  \n",
    "final_train_b.append(vectorizers_b[0].vectorize(Xb_train))\n",
    "final_test_b.append(vectorizers_b[0].vectorize(Xb_test))\n",
    "classifiers_b[0].fit(final_train_b[0],Yb_train)\n",
    "print(\"Done\")\n",
    "classifiers_b[0].score(final_train_b[0], Yb_train, final_test_b[0], Yb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQ_1HxhklyFF"
   },
   "source": [
    "Subtask C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C2MkuedIlyFG",
    "outputId": "a3119696-fbeb-4509-be3e-8e9f46ca36a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Data: 13241it [00:00, 280565.30it/s]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 279it [00:00, 2788.73it/s]\u001b[A\n",
      "Tokenization: 634it [00:00, 2978.01it/s]\u001b[A\n",
      "Tokenization: 990it [00:00, 3131.56it/s]\u001b[A\n",
      "Tokenization: 1337it [00:00, 3224.32it/s]\u001b[A\n",
      "Tokenization: 1677it [00:00, 3274.49it/s]\u001b[A\n",
      "Tokenization: 1981it [00:00, 3198.08it/s]\u001b[A\n",
      "Tokenization: 2288it [00:00, 3156.93it/s]\u001b[A\n",
      "Tokenization: 2583it [00:00, 3064.47it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 2713it [00:00, 35534.96it/s]\u001b[A\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 1it [00:01,  1.53s/it]\u001b[A\n",
      "Lemmatization: 62it [00:01,  1.07s/it]\u001b[A\n",
      "Lemmatization: 118it [00:01,  1.33it/s]\u001b[A\n",
      "Lemmatization: 180it [00:01,  1.90it/s]\u001b[A\n",
      "Lemmatization: 242it [00:01,  2.71it/s]\u001b[A\n",
      "Lemmatization: 309it [00:02,  3.87it/s]\u001b[A\n",
      "Lemmatization: 373it [00:02,  5.52it/s]\u001b[A\n",
      "Lemmatization: 442it [00:02,  7.85it/s]\u001b[A\n",
      "Lemmatization: 520it [00:02, 11.17it/s]\u001b[A\n",
      "Lemmatization: 590it [00:02, 15.84it/s]\u001b[A\n",
      "Lemmatization: 656it [00:02, 22.38it/s]\u001b[A\n",
      "Lemmatization: 726it [00:02, 31.53it/s]\u001b[A\n",
      "Lemmatization: 792it [00:02, 44.00it/s]\u001b[A\n",
      "Lemmatization: 855it [00:02, 59.73it/s]\u001b[A\n",
      "Lemmatization: 910it [00:03, 80.39it/s]\u001b[A\n",
      "Lemmatization: 961it [00:03, 106.91it/s]\u001b[A\n",
      "Lemmatization: 1030it [00:03, 143.21it/s]\u001b[A\n",
      "Lemmatization: 1097it [00:03, 187.38it/s]\u001b[A\n",
      "Lemmatization: 1165it [00:03, 239.28it/s]\u001b[A\n",
      "Lemmatization: 1238it [00:03, 299.49it/s]\u001b[A\n",
      "Lemmatization: 1303it [00:03, 357.08it/s]\u001b[A\n",
      "Lemmatization: 1368it [00:03, 404.40it/s]\u001b[A\n",
      "Lemmatization: 1431it [00:03, 426.43it/s]\u001b[A\n",
      "Lemmatization: 1490it [00:04, 417.87it/s]\u001b[A\n",
      "Lemmatization: 1554it [00:04, 464.76it/s]\u001b[A\n",
      "Lemmatization: 1625it [00:04, 517.55it/s]\u001b[A\n",
      "Lemmatization: 1687it [00:04, 544.50it/s]\u001b[A\n",
      "Lemmatization: 1757it [00:04, 580.54it/s]\u001b[A\n",
      "Lemmatization: 1823it [00:04, 602.12it/s]\u001b[A\n",
      "Lemmatization: 1895it [00:04, 631.81it/s]\u001b[A\n",
      "Lemmatization: 1962it [00:04, 623.66it/s]\u001b[A\n",
      "Lemmatization: 2027it [00:04, 575.49it/s]\u001b[A\n",
      "Lemmatization: 2087it [00:05, 551.69it/s]\u001b[A\n",
      "Lemmatization: 2144it [00:05, 548.64it/s]\u001b[A\n",
      "Lemmatization: 2201it [00:05, 550.38it/s]\u001b[A\n",
      "Lemmatization: 2259it [00:05, 556.32it/s]\u001b[A\n",
      "Lemmatization: 2316it [00:05, 531.26it/s]\u001b[A\n",
      "Lemmatization: 2370it [00:05, 530.47it/s]\u001b[A\n",
      "Lemmatization: 2426it [00:05, 534.84it/s]\u001b[A\n",
      "Lemmatization: 2490it [00:05, 561.09it/s]\u001b[A\n",
      "Lemmatization: 2548it [00:05, 559.71it/s]\u001b[A\n",
      "Lemmatization: 2605it [00:06, 502.19it/s]\u001b[A\n",
      "Lemmatization: 2657it [00:06, 475.66it/s]\u001b[A\n",
      "Lemmatization: 2711it [00:06, 490.26it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:07<00:00,  1.55s/it]\n",
      "Preprocessing:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Tokenization: 0it [00:00, ?it/s]\u001b[A\n",
      "Tokenization: 268it [00:00, 2670.81it/s]\u001b[A\n",
      "Tokenization: 479it [00:00, 2472.91it/s]\u001b[A\n",
      "Tokenization: 818it [00:00, 2689.24it/s]\u001b[A\n",
      "Preprocessing:  33%|███▎      | 1/3 [00:00<00:00,  2.49it/s]\n",
      "Stopwords Removal: 0it [00:00, ?it/s]\u001b[A\n",
      "Stopwords Removal: 1163it [00:00, 39840.37it/s]\u001b[A\n",
      "Lemmatization: 0it [00:00, ?it/s]\u001b[A\n",
      "Lemmatization: 55it [00:00, 536.98it/s]\u001b[A\n",
      "Lemmatization: 97it [00:00, 494.22it/s]\u001b[A\n",
      "Lemmatization: 156it [00:00, 517.46it/s]\u001b[A\n",
      "Lemmatization: 214it [00:00, 534.55it/s]\u001b[A\n",
      "Lemmatization: 264it [00:00, 518.91it/s]\u001b[A\n",
      "Lemmatization: 308it [00:00, 474.92it/s]\u001b[A\n",
      "Lemmatization: 361it [00:00, 486.78it/s]\u001b[A\n",
      "Lemmatization: 427it [00:00, 528.29it/s]\u001b[A\n",
      "Lemmatization: 492it [00:00, 553.27it/s]\u001b[A\n",
      "Lemmatization: 547it [00:01, 502.63it/s]\u001b[A\n",
      "Lemmatization: 598it [00:01, 481.36it/s]\u001b[A\n",
      "Lemmatization: 670it [00:01, 532.65it/s]\u001b[A\n",
      "Lemmatization: 726it [00:01, 510.84it/s]\u001b[A\n",
      "Lemmatization: 779it [00:01, 473.64it/s]\u001b[A\n",
      "Lemmatization: 835it [00:01, 496.42it/s]\u001b[A\n",
      "Lemmatization: 887it [00:01, 481.69it/s]\u001b[A\n",
      "Lemmatization: 937it [00:01, 478.42it/s]\u001b[A\n",
      "Lemmatization: 986it [00:01, 441.04it/s]\u001b[A\n",
      "Lemmatization: 1041it [00:02, 467.38it/s]\u001b[A\n",
      "Lemmatization: 1089it [00:02, 448.69it/s]\u001b[A\n",
      "Lemmatization: 1135it [00:02, 444.42it/s]\u001b[A\n",
      "Preprocessing: 100%|██████████| 3/3 [00:02<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 2713/2713 [00:00<00:00, 32364.73it/s]\n",
      "Finalizing: 100%|██████████| 2713/2713 [00:00<00:00, 3331.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Embeddings from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 1163/1163 [00:00<00:00, 34817.07it/s]\n",
      "Finalizing: 100%|██████████| 1163/1163 [00:00<00:00, 3173.57it/s]\n",
      "/home/sahil/anaconda3/envs/ire_project/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Training Data Score: 0.8363435311463324\n",
      "Test Data Score: 0.6534823731728289\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'micro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ee8849d961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mclassifiers_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mclassifiers_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_test_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/OffensEval-master/offenseval-master/Preprocessor.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Data Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'micro' is not defined"
     ]
    }
   ],
   "source": [
    "final_test_c = []\n",
    "final_train_c = []\n",
    "dr_c = DataReader('./datasets/training-v1/offenseval-training-v1.tsv', 'C')\n",
    "data_c,labels_c = dr_c.get_labelled_data()\n",
    "Xc_train,Xc_test,Yc_train,Yc_test = split(data_c,labels_c,test_size=0.3)\n",
    "preprocessors_c = [('remove_stopwords','lemmatize')]\n",
    "vectorizers_c = ['glove']\n",
    "classifiers_c = [('SVC',{'C':10,'kernel':'rbf'})]\n",
    "preprocessors_c[0] = Preprocessor(preprocessors_c[0])\n",
    "vectorizers_c[0] = Vectorizer(vectorizers_c[0])\n",
    "classifiers_c[0] = Classifier(*classifiers_c[0])\n",
    "Xc_train = preprocessors_c[0].clean(Xc_train)\n",
    "Xc_test = preprocessors_c[0].clean(Xc_test)  \n",
    "final_train_c.append(vectorizers_c[0].vectorize(Xc_train))\n",
    "final_test_c.append(vectorizers_c[0].vectorize(Xc_test))\n",
    "classifiers_c[0].fit(final_train_c[0],Yc_train)\n",
    "print(\"Done\")\n",
    "classifiers_c[0].score(final_train_c[0], Yc_train, final_test_c[0], Yc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "offenseval.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
